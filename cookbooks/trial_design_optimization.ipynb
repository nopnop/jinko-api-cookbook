{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical trial deisgn optimization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cookbook specifics imports\n",
    "import jinko_helpers as jinko\n",
    "import json\n",
    "import io\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from modAL.models import BayesianOptimizer\n",
    "from modAL.acquisition import max_EI\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import scipy.stats as st\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import zipfile\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jinko trial short ID, URL pattern is \"https://jinko.ai/{trial_sid}\"\n",
    "trial_sid = \"tr-HLRF-b0zW\"\n",
    "# Outcome ID\n",
    "outcome_name = \"tumorBurdenChangeFromBaseline.tend\"\n",
    "# control and treated arm IDs\n",
    "control_arm_id = \"sc-1-10\"\n",
    "treated_arm_id = \"iv-1-10\"\n",
    "\n",
    "\"\"\"\n",
    "Sample size parameters\n",
    "\"\"\"\n",
    "alpha = 0.05\n",
    "beta = 0.2\n",
    "\n",
    "\"\"\" \n",
    "Features sorted by descending order of importance (as per tornado or Random Forest analysis for instance)\n",
    "See \"sensitivity_analysis.ipynb\"\n",
    "\"\"\"\n",
    "all_features = [\n",
    "    \"initialTumorBurden.tmin\",\n",
    "    \"ec50Drug.tmin\",\n",
    "    \"Blood.tmin\",\n",
    "    \"Tissue.tmin\",\n",
    "    \"kClearanceDrug.tmin\",\n",
    "]\n",
    "\n",
    "\"\"\" Minimal size of the filtered responder vpop\n",
    "(number of times the required sample size should be included in the responder vp in terms of nb of patients)\n",
    "Will be evaluated for each set of eligibility criteria\n",
    "\"\"\"\n",
    "min_resp_vp_size = 10\n",
    "\n",
    "\"\"\"\n",
    "Objective function weights\n",
    "\"\"\"\n",
    "# weight of gross efficacy\n",
    "efficacy_wt = 1\n",
    "# weight of standard deviation of gross efficacy\n",
    "efficacy_sd_wt = 2\n",
    "\n",
    "\"\"\"\n",
    "Boostrapping parameters\n",
    "\"\"\"\n",
    "## Number of bootstraps for gross efficacy dispersion estimation\n",
    "num_bootstraps = 50\n",
    "## Seed for all random processes\n",
    "cookbook_seed = 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions\n",
    "### Sample size computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of sample size computation formulas\n",
    "## Sample size for two independent samples, continuous outcome\n",
    "def sample_size_continuous_outcome(alpha, beta, diff_btw_groups, sd_outcome, dropout=0):\n",
    "    z_alpha = st.norm.ppf(1 - (alpha / 2))\n",
    "    z_beta = st.norm.ppf(1 - beta)\n",
    "\n",
    "    return (2 * ((z_alpha + z_beta) / (abs(diff_btw_groups) / sd_outcome)) ** 2) / (\n",
    "        1 - dropout\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective function to be maximized\n",
    "def objective_function(\n",
    "    efficacy,\n",
    "    efficacy_sd,\n",
    "):\n",
    "    return (efficacy * efficacy_wt - efficacy_sd * efficacy_sd_wt) / (\n",
    "        efficacy_wt + efficacy_sd_wt\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading trial results from Jinko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jinko.initialize()\n",
    "\n",
    "# Convert short id to core item id\n",
    "trial_core_item_id = jinko.get_core_item_id(trial_sid, 1)\n",
    "\n",
    "# List all trial versions\n",
    "# https://doc.jinko.ai/api/#/paths/core-v2-trial_manager-trial-trialId--status/get\n",
    "response = jinko.make_request(\n",
    "    f'/core/v2/trial_manager/trial/{trial_core_item_id[\"id\"]}/status'\n",
    ")\n",
    "versions = response.json()\n",
    "\n",
    "# Get the latest completed version\n",
    "try:\n",
    "    latest_completed_version = next(\n",
    "        (item for item in versions if item[\"status\"] == \"completed\"), None\n",
    "    )\n",
    "    if latest_completed_version is None:\n",
    "        raise Exception(\"No completed trial version found\")\n",
    "    else:\n",
    "        simulation_id = latest_completed_version[\"simulationId\"]\n",
    "        trial_core_item_id = simulation_id[\"coreItemId\"]\n",
    "        trial_snapshot_id = simulation_id[\"snapshotId\"]\n",
    "except Exception as e:\n",
    "    print(f\"Error processing trial versions: {e}\")\n",
    "    raise\n",
    "\n",
    "# https://doc.jinko.ai/api/#/paths/core-v2-trial_manager-trial-trialId--snapshots--trialIdSnapshot--results_summary/get\n",
    "response = jinko.make_request(\n",
    "    f\"/core/v2/trial_manager/trial/{trial_core_item_id}/snapshots/{trial_snapshot_id}/results_summary\",\n",
    "    method=\"GET\",\n",
    ")\n",
    "response_summary = json.loads(response.content)\n",
    "\n",
    "# Retrieving scalar results\n",
    "json_data = {\n",
    "    \"trialId\": {\"coreItemId\": trial_core_item_id, \"snapshotId\": trial_snapshot_id}\n",
    "}\n",
    "\n",
    "# https://doc.jinko.ai/api/#/paths/core-v2-result_manager-scalars_summary/post\n",
    "response = jinko.make_request(\n",
    "    path=\"/core/v2/result_manager/trial_visualization\",\n",
    "    method=\"POST\",\n",
    "    json=json_data,\n",
    ")\n",
    "\n",
    "# https://doc.jinko.ai/api/#/paths/core-v2-result_manager-scalars_summary/post\n",
    "response = jinko.make_request(\n",
    "    path=\"/core/v2/result_manager/scalars_summary\",\n",
    "    method=\"POST\",\n",
    "    json={\n",
    "        \"select\": all_features + [outcome_name],\n",
    "        \"trialId\": latest_completed_version[\"simulationId\"],\n",
    "    },\n",
    ")\n",
    "archive = zipfile.ZipFile(io.BytesIO(response.content))\n",
    "filename = archive.namelist()[0]\n",
    "\n",
    "csv_scalars = archive.read(filename).decode(\"utf-8\")\n",
    "\n",
    "scalars_dtf = pd.read_csv(io.StringIO(csv_scalars))\n",
    "print(\"Number of rows in the initial table:\", len(scalars_dtf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembling the dataframes of interest\n",
    "cross_arm_scalars = scalars_dtf.loc[scalars_dtf[\"armId\"] == \"crossArms\"].pivot(\n",
    "    index=\"patientId\", columns=\"scalarId\", values=\"value\"\n",
    ")\n",
    "control_arm_scalars = scalars_dtf.loc[scalars_dtf[\"armId\"] == control_arm_id].pivot(\n",
    "    index=\"patientId\", columns=\"scalarId\", values=\"value\"\n",
    ")\n",
    "treated_arm_scalars = scalars_dtf.loc[scalars_dtf[\"armId\"] == treated_arm_id].pivot(\n",
    "    index=\"patientId\", columns=\"scalarId\", values=\"value\"\n",
    ")\n",
    "control_arm_scalars = pd.merge(\n",
    "    left=cross_arm_scalars, right=control_arm_scalars, how=\"left\", on=\"patientId\"\n",
    ")\n",
    "treated_arm_scalars = pd.merge(\n",
    "    left=cross_arm_scalars, right=treated_arm_scalars, how=\"left\", on=\"patientId\"\n",
    ")\n",
    "\n",
    "print(f\"cross_arm_scalars table has {len(cross_arm_scalars)} rows\")\n",
    "print(f\"control_arm_scalars table has {len(control_arm_scalars)} rows\")\n",
    "print(f\"treated_arm_scalars table has {len(treated_arm_scalars)} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial net efficacy and sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_control = control_arm_scalars[outcome_name].mean()\n",
    "mean_treated = treated_arm_scalars[outcome_name].mean()\n",
    "initial_net_efficacy = mean_control - mean_treated\n",
    "\n",
    "std_control = control_arm_scalars[outcome_name].std()\n",
    "std_treated = treated_arm_scalars[outcome_name].std()\n",
    "\n",
    "print(f\"This trial has a net efficacy of {initial_net_efficacy:.3g}\")\n",
    "sample_size = math.ceil(\n",
    "    sample_size_continuous_outcome(alpha, beta, initial_net_efficacy, std_control)\n",
    ")\n",
    "print(f\"With beta = {beta}, this means a sample size of {sample_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting absolute benefit vs features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(all_features)\n",
    "features_wrapped = [\"<br>\".join(textwrap.wrap(t, width=30)) for t in all_features]\n",
    "\n",
    "efficacy_df = pd.merge(\n",
    "    control_arm_scalars, treated_arm_scalars[outcome_name], how=\"inner\", on=\"patientId\"\n",
    ")\n",
    "efficacy_df[\"absolute_benefit\"] = (\n",
    "    efficacy_df[f\"{outcome_name}_x\"] - efficacy_df[f\"{outcome_name}_y\"]\n",
    ")\n",
    "efficacy_df = efficacy_df.dropna()\n",
    "max_absolute_benefit = efficacy_df[\"absolute_benefit\"].max()\n",
    "print(f\"Maximum absolute benefit = {max_absolute_benefit:.2g}\")\n",
    "\n",
    "efficacy_df = efficacy_df.sort_values(by=\"absolute_benefit\")\n",
    "fig = make_subplots(\n",
    "    num_features,\n",
    "    num_features,\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=False,\n",
    "    horizontal_spacing=0.03,\n",
    "    vertical_spacing=0.01,\n",
    "    column_titles=features_wrapped,\n",
    "    row_titles=features_wrapped,\n",
    ")\n",
    "\n",
    "for i in range(num_features):  # iterating over rows\n",
    "    for j in range(num_features):  # iterating over columns\n",
    "        if i == j:\n",
    "            x = efficacy_df[all_features[i]]\n",
    "            nx = 10\n",
    "            xs = np.linspace(x.min(), x.max(), nx + 1)\n",
    "            y = []\n",
    "            for k in range(nx):\n",
    "                eff_k = efficacy_df[\n",
    "                    (efficacy_df[all_features[i]] >= xs[k])\n",
    "                    & (efficacy_df[all_features[i]] < xs[k + 1])\n",
    "                ][\"absolute_benefit\"]\n",
    "                y.append(eff_k.mean())\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=xs[:-1] + 0.5 * (xs[1] - xs[0]), y=y, mode=\"lines\"),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "        else:\n",
    "            fig.add_trace(\n",
    "                go.Scattergl(\n",
    "                    x=efficacy_df[all_features[j]],\n",
    "                    y=efficacy_df[all_features[i]],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        size=5,\n",
    "                        color=efficacy_df[\"absolute_benefit\"],\n",
    "                        opacity=efficacy_df[\"absolute_benefit\"] / max_absolute_benefit,\n",
    "                        coloraxis=\"coloraxis1\",\n",
    "                    ),\n",
    "                    hoverinfo=\"none\",\n",
    "                ),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "fig.update_annotations(font_size=12)\n",
    "fig.update_coloraxes(\n",
    "    colorbar_title=\"absolute benefit\",\n",
    "    cmin=0.2 * max_absolute_benefit,\n",
    "    cmax=0.7 * max_absolute_benefit,\n",
    "    colorscale=\"rainbow\",\n",
    "    colorbar_thickness=20,\n",
    "    colorbar_title_side=\"right\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    font=dict(size=12),\n",
    "    showlegend=False,\n",
    "    width=1000,\n",
    "    height=900,\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locally averaged absolute benefit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using nearest-neighbor regression, we compute the locally averaged absolute benefit\n",
    "Using a sample_size as number of neighbors, the Chebyshev norm and uniform weighting, this is equivalent\n",
    "to computing the mean of the absolute benefit in a hypercube centered around each point in the feature space\n",
    "and therefore serves a good proxy for the net efficacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsRegressor(\n",
    "    n_neighbors=sample_size, weights=\"uniform\", metric=\"chebyshev\"\n",
    ")\n",
    "neigh.fit(\n",
    "    efficacy_df[all_features].to_numpy(), efficacy_df[\"absolute_benefit\"].to_numpy()\n",
    ")\n",
    "efficacy_df[\"locally_averaged_absolute_benefit\"] = neigh.predict(\n",
    "    efficacy_df[all_features].to_numpy()\n",
    ")\n",
    "min_locally_averaged_absolute_benefit, max_locally_averaged_absolute_benefit = (\n",
    "    efficacy_df[\"locally_averaged_absolute_benefit\"].min(),\n",
    "    efficacy_df[\"locally_averaged_absolute_benefit\"].max(),\n",
    ")\n",
    "\n",
    "\n",
    "efficacy_df = efficacy_df.sort_values(by=\"locally_averaged_absolute_benefit\")\n",
    "fig = make_subplots(\n",
    "    num_features,\n",
    "    num_features,\n",
    "    shared_xaxes=True,\n",
    "    shared_yaxes=False,\n",
    "    horizontal_spacing=0.03,\n",
    "    vertical_spacing=0.01,\n",
    "    column_titles=features_wrapped,\n",
    "    row_titles=features_wrapped,\n",
    ")\n",
    "\n",
    "for i in range(num_features):  # iterating over rows\n",
    "    for j in range(num_features):  # iterating over columns\n",
    "        if i == j:  # do not plot anything on the diagonal\n",
    "            x = efficacy_df[all_features[i]]\n",
    "            nx = 10\n",
    "            xs = np.linspace(x.min(), x.max(), nx + 1)\n",
    "            y = []\n",
    "            for k in range(nx):\n",
    "                eff_k = efficacy_df[\n",
    "                    (efficacy_df[all_features[i]] >= xs[k])\n",
    "                    & (efficacy_df[all_features[i]] < xs[k + 1])\n",
    "                ][\"locally_averaged_absolute_benefit\"]\n",
    "                y.append(eff_k.mean())\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=xs[:-1] + 0.5 * (xs[1] - xs[0]), y=y, mode=\"lines\"),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "        else:\n",
    "            fig.add_trace(\n",
    "                go.Scattergl(\n",
    "                    x=efficacy_df[all_features[j]],\n",
    "                    y=efficacy_df[all_features[i]],\n",
    "                    mode=\"markers\",\n",
    "                    marker=dict(\n",
    "                        size=5,\n",
    "                        color=efficacy_df[\"locally_averaged_absolute_benefit\"],\n",
    "                        opacity=efficacy_df[\"locally_averaged_absolute_benefit\"]\n",
    "                        / max_locally_averaged_absolute_benefit,\n",
    "                        coloraxis=\"coloraxis1\",\n",
    "                    ),\n",
    "                    hoverinfo=\"none\",\n",
    "                ),\n",
    "                row=i + 1,\n",
    "                col=j + 1,\n",
    "            )\n",
    "fig.update_annotations(font_size=12)\n",
    "fig.update_coloraxes(\n",
    "    colorbar_title=\"locally averaged absolute benefit\",\n",
    "    cmin=min_locally_averaged_absolute_benefit,\n",
    "    cmax=max_locally_averaged_absolute_benefit,\n",
    "    colorscale=\"rainbow\",\n",
    "    colorbar_thickness=15,\n",
    "    colorbar_title_side=\"right\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    font=dict(size=14),\n",
    "    showlegend=False,\n",
    "    width=1000,\n",
    "    height=900,\n",
    "    template=\"plotly_white\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 3\n",
    "features = all_features[:num_features]\n",
    "print(f\"Selected features: {features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gross efficacy tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_row(x_vec, columns):\n",
    "    return {columns[i]: x for i, x in enumerate(x_vec)}\n",
    "\n",
    "\n",
    "def lower_bound(d):\n",
    "    return f\"{d}_lower_bound\"\n",
    "\n",
    "\n",
    "def width(d):\n",
    "    return f\"{d}_width\"\n",
    "\n",
    "\n",
    "feature_bounds = {\n",
    "    d: (cross_arm_scalars.min(axis=0)[d], cross_arm_scalars.max(axis=0)[d])\n",
    "    for d in features\n",
    "}\n",
    "column_bounds = {}\n",
    "for d in features:\n",
    "    (d_min, d_max) = feature_bounds[d]\n",
    "    column_bounds[lower_bound(d)] = (d_min, d_max - 0.1 * (d_max - d_min))\n",
    "    column_bounds[width(d)] = (0.1 * (d_max - d_min), d_max - d_min)\n",
    "\n",
    "dim = len(column_bounds)\n",
    "column_keys = list(column_bounds.keys())\n",
    "\n",
    "\n",
    "def filter_conditions(row):\n",
    "    conditions = [\n",
    "        (cross_arm_scalars[d] >= row[lower_bound(d)])\n",
    "        & (cross_arm_scalars[d] <= row[lower_bound(d)] + row[width(d)])\n",
    "        for d in features\n",
    "    ]\n",
    "    return np.logical_and.reduce(conditions)\n",
    "\n",
    "\n",
    "def group_size(row):\n",
    "    return len(cross_arm_scalars[filter_conditions(row)])\n",
    "\n",
    "\n",
    "def filter(row):\n",
    "    if group_size(row) <= sample_size * min_resp_vp_size:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "def score(row, seed=cookbook_seed, n_boot=num_bootstraps, verbose=False):\n",
    "    mean_efficacy, sd_efficacy = gross_efficacy(row, seed=seed, n_boot=n_boot)\n",
    "    if verbose:\n",
    "        print(f\"mean, std of gross efficacy = {mean_efficacy:.3g}, {sd_efficacy:.2g}\")\n",
    "    if mean_efficacy and sd_efficacy:\n",
    "        return objective_function(\n",
    "            efficacy=mean_efficacy,\n",
    "            efficacy_sd=sd_efficacy,\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def gross_efficacy(row, seed=cookbook_seed, n_boot=num_bootstraps):\n",
    "    # A reproducible random generator whose seed depends on the top-level cookbook seed AND the evaluated design row\n",
    "    rng = np.random.default_rng([seed, abs(hash(frozenset(row.items())))])\n",
    "    # Creating the corresponding control and treated filtered dataset\n",
    "    control_filtered = control_arm_scalars[filter_conditions(row)][outcome_name]\n",
    "    treated_filtered = treated_arm_scalars[filter_conditions(row)][outcome_name]\n",
    "    if len(control_filtered) <= sample_size * min_resp_vp_size:\n",
    "        return None, None\n",
    "    bootstrap_gross_efficacies = []\n",
    "    all_indices = np.arange(len(control_filtered))\n",
    "    for _ in range(n_boot):\n",
    "        # pick 2 * sample_size indices at random from the full dataset\n",
    "        shuffled_indices = rng.choice(all_indices, size=2 * sample_size, replace=False)\n",
    "        # first sample_size indices for the control sub-group\n",
    "        ctrl_mean = control_filtered.iloc[shuffled_indices[:sample_size]].mean()\n",
    "        # next sample_size indices for the treated sub-group\n",
    "        trtd_mean = treated_filtered.iloc[\n",
    "            shuffled_indices[sample_size : 2 * sample_size]\n",
    "        ].mean()\n",
    "        bootstrap_gross_efficacies.append(ctrl_mean - trtd_mean)\n",
    "\n",
    "    return np.mean(bootstrap_gross_efficacies), np.std(\n",
    "        bootstrap_gross_efficacies, ddof=1\n",
    "    )\n",
    "\n",
    "\n",
    "def net_efficacy(row):\n",
    "    control_filtered = control_arm_scalars[filter_conditions(row)]\n",
    "    treated_filtered = treated_arm_scalars[filter_conditions(row)]\n",
    "    efficacy_df = pd.merge(\n",
    "        control_filtered, treated_filtered[outcome_name], how=\"inner\", on=\"patientId\"\n",
    "    )\n",
    "    efficacy_df[\"net_efficacy\"] = (\n",
    "        efficacy_df[f\"{outcome_name}_x\"] - efficacy_df[f\"{outcome_name}_y\"]\n",
    "    )\n",
    "    mean_efficacy, std_efficacy = (\n",
    "        efficacy_df[\"net_efficacy\"].mean(),\n",
    "        efficacy_df[\"net_efficacy\"].std(),\n",
    "    )\n",
    "    return (mean_efficacy, std_efficacy)\n",
    "\n",
    "\n",
    "def format_eligibility_criteria(row):\n",
    "    s = \"\"\n",
    "    for f in features:\n",
    "        s += f\"\\n {row[lower_bound(f)]:.3g} <= {f} <= {row[lower_bound(f)] + row[width(f)]:.3g}\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating design of experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = st.qmc.Sobol(dim, scramble=False)\n",
    "m = 14\n",
    "print(f\"Full design size = {2**m}\")\n",
    "scaled_samples = sampler.random_base2(m)  # generates 2**m points\n",
    "samples = st.qmc.scale(\n",
    "    scaled_samples,\n",
    "    [column_bounds[c][0] for c in column_keys],\n",
    "    [column_bounds[c][1] for c in column_keys],\n",
    ")\n",
    "\n",
    "filtered_indices = []\n",
    "for i, x in enumerate(samples):\n",
    "    row = to_row(x, columns=column_keys)\n",
    "    if filter(row):\n",
    "        filtered_indices.append(i)\n",
    "print(f\"Number of admissible points = {len(filtered_indices)} / {samples.shape[0]} \")\n",
    "filtered_samples = samples[filtered_indices, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exhaustive search (turned off by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn this flag to True to run an exhaustive search\n",
    "# Warning: this may take a lot of time to run, approx. 15s per 1000 admissible points\n",
    "exhaustive_search = True\n",
    "if exhaustive_search:\n",
    "    y = [score(to_row(x, columns=column_keys)) for x in filtered_samples]\n",
    "    exhaustive_i_max, exhaustive_y_max = np.argmax(y), np.amax(y)\n",
    "    exhaustive_best_criteria = to_row(\n",
    "        filtered_samples[exhaustive_i_max], columns=column_keys\n",
    "    )\n",
    "    print(\n",
    "        f\"Exhaustive search: best row index = {exhaustive_i_max}, best score = {exhaustive_y_max:.3g}\"\n",
    "    )\n",
    "    print(\n",
    "        \"Exhaustive search best eligibility criteria:\\n\"\n",
    "        + format_eligibility_criteria(exhaustive_best_criteria)\n",
    "    )\n",
    "else:\n",
    "    exhaustive_best_criteria = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full design length\n",
    "N = filtered_samples.shape[0]\n",
    "\n",
    "# training size\n",
    "n = 128\n",
    "scaled_filtered = st.qmc.scale(\n",
    "    filtered_samples,\n",
    "    [column_bounds[c][0] for c in column_keys],\n",
    "    [column_bounds[c][1] for c in column_keys],\n",
    "    reverse=True,\n",
    ")\n",
    "# display(scaled_filtered)\n",
    "X_training = scaled_filtered[:n, :]\n",
    "y_training = [\n",
    "    score(to_row(filtered_samples[i, :], columns=column_keys)) for i in range(n)\n",
    "]\n",
    "print(f\"Training best score: {np.amax(y_training):.3g}\")\n",
    "\n",
    "kernel = Matern(length_scale=1.0)\n",
    "regressor = GaussianProcessRegressor(kernel=kernel)\n",
    "optimizer = BayesianOptimizer(\n",
    "    estimator=regressor,\n",
    "    X_training=X_training,\n",
    "    y_training=y_training,\n",
    "    query_strategy=max_EI,\n",
    ")\n",
    "X_max, y_max = optimizer.get_max()\n",
    "\n",
    "# number of Bayesian Optimization iterations\n",
    "num_queries = 50\n",
    "queried = []\n",
    "y_queried = []\n",
    "for n_query in range(num_queries):\n",
    "    (query_idx,), query_inst = optimizer.query(scaled_filtered)\n",
    "    queried.append(query_idx)\n",
    "    y = score(to_row(filtered_samples[query_idx, :], columns=column_keys))\n",
    "    y_queried.append(y)\n",
    "    optimizer.teach(scaled_filtered[query_idx, :].reshape(1, -1), np.array([y]))\n",
    "    X_max, y_max = optimizer.get_max()\n",
    "    if n_query % 10 == 0:\n",
    "        print(\n",
    "            f\"iteration = {n_query}, queried index: {query_idx}, score = {y:.2g}, best ever score = {y_max:.3g}\"\n",
    "        )\n",
    "    if exhaustive_search:\n",
    "        if abs(y_max - exhaustive_y_max) < 1e-3:\n",
    "            print(\n",
    "                f\"iteration = {n_query}, queried index: {query_idx}, score = {y:.3g} is close enough to exhaustive search maximum, let's stop\"\n",
    "            )\n",
    "            break\n",
    "bo_best_x = st.qmc.scale(\n",
    "    np.array([X_max]),\n",
    "    [column_bounds[c][0] for c in column_keys],\n",
    "    [column_bounds[c][1] for c in column_keys],\n",
    ")[0, :]\n",
    "bo_best_criteria = to_row(bo_best_x, columns=column_keys)\n",
    "bo_best_score = score(bo_best_criteria, verbose=True)\n",
    "print(f\"Bayesian Optimization best score = {bo_best_score:.3g}\")\n",
    "print(\n",
    "    \"Bayesian Optimization best eligibility criteria:\\n\"\n",
    "    + format_eligibility_criteria(bo_best_criteria)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score landscape around Bayesian Optimization best estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 21\n",
    "xs = np.linspace(0.8, 1.2, nx)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for j in range(len(bo_best_x)):\n",
    "    u = bo_best_x.copy()\n",
    "    ys = []\n",
    "    for k in range(nx):\n",
    "        u[j] = bo_best_x[j] * xs[k]\n",
    "        ys.append(score(to_row(u, columns=column_keys)))\n",
    "    plt.plot(xs, ys, label=column_keys[j])\n",
    "plt.xlabel(\"parameter multiplicative factor\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are we lucky? Pure Monte Carlo\n",
    "### Does a simple Monte Carlo fare better than Bayesian Optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples to draw\n",
    "num_mc = 10000\n",
    "rng = np.random.default_rng([cookbook_seed])\n",
    "X_mc = rng.uniform(low=0, high=1, size=(num_mc, dim))\n",
    "for j in range(dim):\n",
    "    (min_x, max_x) = (\n",
    "        column_bounds[column_keys[j]][0],\n",
    "        column_bounds[column_keys[j]][1],\n",
    "    )\n",
    "    X_mc[:, j] = min_x + (max_x - min_x) * X_mc[:, j]\n",
    "\n",
    "filtered_indices = []\n",
    "for i, x in enumerate(X_mc):\n",
    "    row = to_row(x, columns=column_keys)\n",
    "    if filter(row):\n",
    "        filtered_indices.append(i)\n",
    "\n",
    "print(f\"Number of admissible points = {len(filtered_indices)} / {X_mc.shape[0]} \")\n",
    "X_mc_filtered = X_mc[filtered_indices, :]\n",
    "y_mc = np.array([score(to_row(x, columns=column_keys)) for x in X_mc_filtered])\n",
    "mc_i_max = np.argmax(y_mc)\n",
    "mc_best_criteria = to_row(X_mc_filtered[mc_i_max], columns=column_keys)\n",
    "mc_best_score = score(mc_best_criteria, verbose=True)\n",
    "\n",
    "print(f\"MC best score = {mc_best_score:.3g}\")\n",
    "print(\"MC best eligibility criteria:\\n\" + format_eligibility_criteria(mc_best_criteria))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(criteria, n_boot):\n",
    "    num_responding = group_size(criteria)\n",
    "    print(f\"  Number of unique best responding patients: {num_responding}\")\n",
    "    mean_gross_efficacy, sd_gross_efficacy = gross_efficacy(criteria, n_boot=n_boot)\n",
    "    print(\n",
    "        f\"  Mean of gross efficacy (with {n_boot} bootstraps) = {mean_gross_efficacy:.3g}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"  Standard deviation of gross efficacy (with {n_boot} bootstraps) = {sd_gross_efficacy:.2g}\"\n",
    "    )\n",
    "    control_filtered = control_arm_scalars[filter_conditions(criteria)][outcome_name]\n",
    "    treated_filtered = treated_arm_scalars[filter_conditions(criteria)][outcome_name]\n",
    "    best_responders_net_efficacy = control_filtered.mean() - treated_filtered.mean()\n",
    "    print(\n",
    "        f\"  Net efficacy in the population of best responders = {best_responders_net_efficacy:.3g}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    --> gain in net efficacy = {best_responders_net_efficacy - initial_net_efficacy:.3g} (+{(best_responders_net_efficacy - initial_net_efficacy)/initial_net_efficacy:.0%})\"\n",
    "    )\n",
    "    control_outcome_std = control_filtered.std()\n",
    "    best_sample_size = math.ceil(\n",
    "        sample_size_continuous_outcome(\n",
    "            alpha, beta, best_responders_net_efficacy, control_outcome_std\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        f\"  Required sample size in population of best responders = {best_sample_size}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"    --> gain in required sample size = {sample_size - best_sample_size} (-{(sample_size - best_sample_size)/sample_size:.0%})\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(f\"Net efficacy in initial population = {initial_net_efficacy:.3g}\")\n",
    "print(f\"Required sample size in initial population = {sample_size}\")\n",
    "\n",
    "if exhaustive_best_criteria:\n",
    "    print(\"\\nExhaustive search:\")\n",
    "    print_report(exhaustive_best_criteria, n_boot=50)\n",
    "if bo_best_criteria:\n",
    "    print(\"\\nBayesian Optimization:\")\n",
    "    print_report(bo_best_criteria, n_boot=50)\n",
    "if mc_best_criteria:\n",
    "    print(\"\\nMonte Carlo:\")\n",
    "    print_report(mc_best_criteria, n_boot=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jinko-api-cookbook-6UM2g0mU-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
